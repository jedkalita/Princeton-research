import math
import numpy as np
from scipy.stats import rv_discrete
import threading
import time
import logging
import random

logging.basicConfig(level=logging.DEBUG,
                    format='(%(threadName)-9s) %(message)s', )

class Environment: #the class that will be simulating the adversary in terms of generating rewards back and forth with
    #each player
    def __init__(self, numPlayers, N):
        self.lock = threading.Lock()
        self.numPlayers = numPlayers #no of players in the game
        self.N = N #no of strategies that each player has

    def generateRewards(self, playerID, pickedStrategy): #generate the cost vector for player playerID
    # who executed strategy pickedStrategy
    #now, we have to literally calculate all of the possible permutations between other players and their strategies
        logging.debug("Waiting for a lock for player = %d" % playerID)
        self.lock.acquire()
        try:
            logging.debug("Acquired a lock for player = %d" % playerID)
            print("Player %d has executed strategy number %d" % (playerID, pickedStrategy))
            totalScenarios = (self.numPlayers - 1) * N #the different scenarios of strategies per remaining players that could
            #happen
            expectedCost = 0
            if totalScenarios == 0:
                expectedCost = input("Single Player game. Enter cost for picked strategy above: ")
            else:
                for i in range(0, totalScenarios): #each scenario encoding a particular representation of opponents' strategies
                    cost = input("Enter the cost for scenario: %d" % (i + 1)) #prompt the user for cost against a config
                    expectedCost = expectedCost + cost #keep the sum count
                expectedCost = (expectedCost * 1.0) / (totalScenarios * 1.0) #to get the expectation
        finally:
            logging.debug("Released a lock for for player = %d" % playerID)
            self.lock.release()
            return expectedCost #this is the cost needed by player who picked the strategy

class Player:
    def __init__(self, T, N, env):
        self.T = T #to make the matrix
        self.N = N #the number of actions/strategies that the player has
        self.weight = np.ones((self.N, self.T), dtype=float) #each player will have their cost vector indexed acc to the time instance
        #this has been initialized to 1.0 for all, but we really only care about for t=1 for all actions acc to the
        #algorithm
        #self.cost = np.zeros(self.N, self.T) #the cost vector for each individual player that will be getting filled
        #for the time t after they have chosen their action, being updated upon by the environment/adversary
        self.epsilon = math.sqrt(math.log(self.N) / self.T) #the value of learning parameter for the no regret case
        # under known T
        self.env = env #the environment object that all players will play under

    def pickStrategy(self, t): #this will be called for the t+1th time instance after the player has picked a strategy
        #acc to the weight matrix that existed at time t
        weighted_total = sum(self.weight[:, t - 1]) #get the weighted sum of all strategies of the player at time t - 1
        #the above goes in the denominator
        probability = self.weight[:, t - 1] / weighted_total #get the probabilities for the individual weights to
        #randomize over
        values_over = self.weight[:, t - 1] #the values over which we will pick, essentially we only need the index
        #of the strategy, hence the range() function below
        distrib = rv_discrete(values = (range(len(values_over)), probability)) #generate the distribution
        return (distrib.rvs(size = 1)) #pick one randomized choice from the distribution created above and return, this
        #will be the strategy that the player picks, and this index will then be changed based on the cost vector fed
        #by the environment/adversary, who will keep track of the cost of each particular strategy

    def changeWeight(self, pickedStrategy, t, playerID): #given cost vector at time t generated by environment/adversary,
        #change the weight of the pickedStrategy at time t for time t+1
        if t == self.T - 1: #the last time step
            return #just return
        self.weight[pickedStrategy, t + 1] = self.weight[pickedStrategy, t] * math.pow((1 - self.epsilon),
                                                                                       self.env.generateRewards(playerID, pickedStrategy))
        #the multiplicative update formula

def play(players, playerID, t): #to play each game at each time step
    strategyPicked = players[playerID].pickStrategy(t) #pick the strategy
    #now, its time to change the weight of the picked strategy for the next time step by calling the environment's
    #generate rewards within, which will be operated via a lock
    players[playerID].changeWeight(strategyPicked, t, playerID)


if __name__ == '__main__':
    Tstr = input("Enter a time horizon: ")
    T = int(Tstr) #the total number of time divisions, T
    Nstr = input("Enter number of actions/strategies of user: ")
    N = int(Nstr) #total no. of actions
    NumPl = input("No. of players: ")
    NumPl = int(NumPl) #no. of players
    print("Time horizon entered : ", T, ". No. of actions: ", N, ". No. of players: ", NumPl)
    '''epsilon = math.sqrt(math.log(N) / T) #the value of learning parameter for the no regret case under known T
    print("Value of learning parameter: ", epsilon)
    n = np.ones((N,T), dtype=float)
    wt = sum(n[:,1])
    print(n)
    print(n[:,1]/wt)
    val = n[:,1]
    prob = n[:,1]/wt
    distrib = rv_discrete(values=(range(len(val)), prob))
    print(distrib.rvs(size=2))
    n[0,1] = 1
    n[1,1] = 3
    n[2,1] = 5
    print(n)
    wt = sum(n[:,1])
    print(n[:,1]/wt)
    val = n[:,1]
    prob = n[:,1]/wt
    print(prob)
    distrib = rv_discrete(values=(range(len(val)), prob))
    print(distrib.rvs(size=1))
    #print(sum(n[1]))'''
    #initialize the environment variable
    env = Environment(NumPl, N)
    #now, initialize the players in a loop
    players = [] #the list of players
    for i in range(NumPl): #for each player
        players.append(Player(T, NumPl, env)) #each player has been initialized

    for i in range(0, T): #basically at each time step each player executes a new thread of execution for picking a strategy
        #and then the process of generating rewards takes place within a lock so that only one player at any one point
        #in time is executing the environment variable for generating rewards
        for j in range(NumPl): #for each of the players, threads will be generated individually per time step
            t = threading.Thread(target=play, args=(players, j, i))
            t.start()

        #now join all the threads before moving on to the next time step
        logging.debug("Waiting for all threads.")
        main_thread = threading.currentThread()
        for t in threading.enumerate():
            if t is not main_thread:
                t.join()
        print("Moving on to time step %d" % (i + 1))

    print("Game over...")





